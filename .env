# Auth
AUTH_TOKEN=demo-key

# Modelle
MODEL=llama3.2
EMBED_MODEL=nomic-embed-text

# EMBED
MAX_TOKENS_PER_CHUNK=350          
OVERLAP_TOKENS=40
MIN_CHUNK_CHARS=40
EMBED_DIM=768

# Retrieval
RAG_TOPK=12
RAG_RETURN=3
RAG_MMR_LAMBDA=0.5
RAG_MAX_CTX_TOKENS=1500

# LLM-Optionen
NUM_CTX=3072
MAX_TOKENS=160
TEMPERATURE=0.2

# Services (Docker-Netz)
OLLAMA_URL=http://ollama:11434
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=docs

# Ollama-Server Verhalten
OLLAMA_KEEP_ALIVE=12h     # wie lange Modelle im RAM bleiben (z.B. 0, 30m, 2h, 12h)
OLLAMA_NUM_PARALLEL=1    # parallele Ausf√ºhrungen begrenzen (small CPU box)
OLLAMA_MAX_LOADED_MODELS=2  # max. Modelle gleichzeitig im RAM
