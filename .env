# Auth
AUTH_TOKEN=demo-key

# Modelle
MODEL=llama3.2
EMBED_MODEL=nomic-embed-text

# EMBED
MAX_TOKENS_PER_CHUNK=350          
OVERLAP_TOKENS=40
MIN_CHUNK_CHARS=40
EMBED_DIM=768

# Retrieval
RAG_TOPK=3
RAG_RETURN=3
RAG_MAX_CTX_TOKENS=1500 #eigentlich max chars, da kein Tokenizer

# LLM-Optionen
NUM_CTX=3072
MAX_TOKENS=160
TEMPERATURE=0.2

# Services (Docker-Netz)
OLLAMA_URL=http://ollama:11434
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=docs

# Ollama-Server Verhalten
OLLAMA_KEEP_ALIVE=12h    
OLLAMA_NUM_PARALLEL=1    
OLLAMA_MAX_LOADED_MODELS=2 
 

