1) Docker Container bauen
2) Modelle runterladen mit docker-compose exec ollama ollama pull llama3,nomic-embed-text
3) Daten ingesten mit docker-compose exec api python ingest.py
4) OpenWebUI unter http://localhost:3000 öffnen
5) Richtiges Modell(llama3.2) wählen und ausprobieren
Bei Fragen gerne melden!